{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Interface Demo\n",
    "\n",
    "Demo showing how to interface YOLO with the ONNX framework.\n",
    "\n",
    "Packages are availabe under conda environment 'yolov5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment and packages\n",
    "Make sure that we are in conda environment 'yolov5' !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import yolov5\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from yolov5 import YOLOv5\n",
    "#from yolov5 import export\n",
    "\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "data_dir = home_dir + '/../data'\n",
    "\n",
    "# Import yolo's prediction app (detect.py) manually\n",
    "pred_app_path = os.path.dirname(yolov5.__file__) + '/detect.py'\n",
    "pred_spec = importlib.util.spec_from_file_location('detect', pred_app_path)\n",
    "detect = importlib.util.module_from_spec(pred_spec)\n",
    "pred_spec.loader.exec_module(detect)\n",
    "\n",
    "# Import yolo's export app\n",
    "exp_app_path = os.path.dirname(yolov5.__file__) + '/export.py'\n",
    "exp_spec = importlib.util.spec_from_file_location('export', exp_app_path)\n",
    "export = importlib.util.module_from_spec(exp_spec)\n",
    "exp_spec.loader.exec_module(export)\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "#model = YOLOv5(data_dir + '/model/pen-parts/weights/best.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a prediction run as sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-2-28 Python-3.10.13 torch-2.2.1 CPU\n",
      "\n",
      "/Users/johannes/anaconda3/envs/yolov5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fusing layers... \n",
      "Model summary: 214 layers, 7235389 parameters, 0 gradients, 16.6 GFLOPs\n",
      "image 1/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video1_19_jpg.rf.95d0f3f146bb957ce3adb0fbfb4209f0.jpg: 416x416 2 scissorss, 2 toothbrushs, 54.7ms\n",
      "image 2/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video1_1_jpg.rf.adeeb32899aea66fd0441482e1c970df.jpg: 416x416 2 cell phones, 52.6ms\n",
      "image 3/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video1_21_jpg.rf.667854f812a30e379d477e95630a9b3f.jpg: 416x416 2 toothbrushs, 54.7ms\n",
      "image 4/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video1_23_jpg.rf.44bc85206f9fa8609734604c13ea82ec.jpg: 416x416 1 scissors, 52.1ms\n",
      "image 5/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video1_8_jpg.rf.815eb1fbb012e439b92e7c0c7cb93b0f.jpg: 416x416 1 scissors, 2 toothbrushs, 55.6ms\n",
      "image 6/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video3_10_jpg.rf.22ca2c70fa7e19ee9db9314a29062f3d.jpg: 416x416 1 toilet, 3 books, 4 scissorss, 57.5ms\n",
      "image 7/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video3_11_jpg.rf.44b9815a3e2a035825949864ae794ba8.jpg: 416x416 2 books, 3 scissorss, 54.4ms\n",
      "image 8/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video3_13_jpg.rf.70d8f58964b63ba30d840da776b2c267.jpg: 416x416 2 bananas, 2 scissorss, 54.4ms\n",
      "image 9/9 /Users/johannes/repos/data/dataset/pen-parts/valid/images/Video3_14_jpg.rf.4f9c8cef3d5cf106e8286336cad7faf0.jpg: 416x416 3 laptops, 4 scissorss, 55.4ms\n",
      "Speed: 0.2ms pre-process, 54.6ms inference, 0.6ms NMS per image at shape (1, 3, 416, 416)\n",
      "Results saved to \u001b[1m/Users/johannes/repos/BA_Repo/../data/data/prediction/pen-parts3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import yolo's prediction app (detect.py) manually\n",
    "pred_app_path = os.path.dirname(yolov5.__file__) + '/detect.py'\n",
    "spec = importlib.util.spec_from_file_location('detect', pred_app_path)\n",
    "detect = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(detect)\n",
    "\n",
    "# Set-up prediction parameters\n",
    "dataset_name = 'pen-parts'\n",
    "model_path = data_dir + '/model/pen-parts/weights/best.pt'\n",
    "source_dir = data_dir + '/dataset/pen-parts/valid/images'\n",
    "meta_path = data_dir + '/dataset/pen-parts/data.yaml'\n",
    "prediction_path = data_dir + '/data/prediction/'\n",
    "image_size = (416,416)\n",
    "\n",
    "detect.run(weights=model_path, \n",
    "           source=source_dir, \n",
    "           data=meta_path,\n",
    "           imgsz=image_size,\n",
    "           conf_thres=0.25, \n",
    "           project=prediction_path, \n",
    "           name=dataset_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert YOLO's trained model from pt to onnx format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export using the export app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ 2024-2-28 Python-3.10.13 torch-2.2.1 CPU\n",
      "\n",
      "/Users/johannes/anaconda3/envs/yolov5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fusing layers... \n",
      "sun_model summary: 233 layers, 7263185 parameters, 0 gradients\n",
      "WARNING ‚ö†Ô∏è --img-size 900 must be multiple of max stride 32, updating to 928\n",
      "WARNING ‚ö†Ô∏è --img-size 900 must be multiple of max stride 32, updating to 928\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /Users/johannes/repos/BA_Repo/../data/model/planets-gearbox/weights/best.pt with output shape (1, 52983, 9) (14.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 0.9s, saved as /Users/johannes/repos/BA_Repo/../data/model/planets-gearbox/weights/best.onnx (28.5 MB)\n",
      "\n",
      "Export complete (1.7s)\n",
      "Results saved to \u001b[1m/Users/johannes/repos/data/model/planets-gearbox/weights\u001b[0m\n",
      "Detect:          yolov5 predict --weights /Users/johannes/repos/BA_Repo/../data/model/planets-gearbox/weights/best.onnx \n",
      "Validate:        yolov5 val --weights /Users/johannes/repos/BA_Repo/../data/model/planets-gearbox/weights/best.onnx \n",
      "Python:          model = yolov5.load('/Users/johannes/repos/BA_Repo/../data/model/planets-gearbox/weights/best.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/johannes/repos/BA_Repo/../data/model/planets-gearbox/weights/best.onnx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Spcify export parameters\n",
    "batch_size = 1 # num of images to be processed in one run\n",
    "exp_format = ['onnx']\n",
    "opset_version = 10 # the ONNX version to use for export\n",
    "use_dynamic = False # do not allow variable image size as input\n",
    "use_simplify = False # do not use the onnx simplifier\n",
    "image_size = (900,900)\n",
    "\n",
    "model_path = data_dir + '/model/planets-gearbox/weights/best.pt'\n",
    "meta_path = data_dir + '/model/planets-gearbox//data.yaml'\n",
    "\n",
    "export.run(data=meta_path,\n",
    "           weights=model_path,\n",
    "           imgsz=image_size,\n",
    "           batch_size=batch_size,\n",
    "           opset=opset_version,\n",
    "           include=exp_format,\n",
    "           dynamic=use_dynamic,\n",
    "           simplify=use_simplify)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "DEPRECATED"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johannes/anaconda3/envs/yolov5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0...\n",
      "/Users/johannes/anaconda3/envs/yolov5/lib/python3.10/site-packages/yolov5/models/common.py:522: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "/Users/johannes/anaconda3/envs/yolov5/lib/python3.10/site-packages/yolov5/models/yolo.py:64: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export failure ‚ùå 0.9s: 'int' object is not iterable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This will be the (fixed) format for predictions with the exported model.\n",
    "# Use the same format as the one for the training images.\n",
    "batch_size = 1 # num of images to be processed in one run\n",
    "trace_im = (torch.randn(batch_size, 3, image_size[0], image_size[1], requires_grad=False),)\n",
    "\n",
    "file_export = Path('plantes-gearbox.onnx') # filename as pathlib object\n",
    "opset_version = 10 # the ONNX version to use for export\n",
    "use_dynamic = False # do not allow variable image size as input\n",
    "use_simplify = False # do not use the onnx simplifier\n",
    "\n",
    "# Export the model to ONNX\n",
    "export.export_onnx(model.model, trace_im, file_export, opset_version, use_dynamic, use_simplify) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
